#Assignment 3: Take Home Exam
###Julia Manning | 12875795

##Selected Question: Question 3
*Marketing or advertising companies would be very interested in being able to predict whether a Twitter message will spread as a meme 
or not, and even better, construct it so that it will spread. Why is this a hard problem to solve? Describe two approaches using data 
analytics to predict whether a tweet will go viral or not. How would you validate these approaches? Discuss the ethical and social 
consequences of this study.*

###1. __Why is this a hard problem to solve?__
There are multiple aspects that need to be considered when solving this problem. One of the major problems is retrieving the data. 
According to Sayce (2019), there are roughly 500 million tweets sent every day, therefore making it difficult to retrieve such a large 
number of data within a short amount of time, as the life-span of each tweet is relatively short, only lasting 24 minutes before it 
becomes less relevant. The tweets are also in a non-structured format which makes it more difficult to retrieve the data compared to the 
traditional structured format, however Twitter does supply information in relation to API calls. These API calls make the retrieval of 
data easier for the developer and consist of multiple options such as:
- Search for historical tweets,
- Monitoring account activity,
- Filter real-time tweets, 
- Build customised Direct Messages, and
- Embedding tweets and timelines into websites (Twitter, 2019).

The second problem is handling the large amounts of data made available. With this problem comes the flow-on effect of a large 
computational cost. The proposed method would have to not only convert the unstructured data into a structured and readable format, but 
it would also have to determine what is deemed a "viral" tweet in creating relationships to the data with ther factors. These factors 
include the number of followers, the number of retweets and the number of mentions for each individual user or their tweets. Due to the 
large size of the assumed dataset, this process would take a significant amount of time to compute and process, in order to receive any 
output.

The third problem is filtering the large dataset into a reasonable and legitimate size without placing any bias or manipulation strategies 
on the dataset. As the dataset is large in size, there would need to be some sort of filtering applied not only the the contents of the 
tweets, but the numbers of tweets as well. The developer would need to make the decision on which the filtering occurred and what tweets 
would be included. This relates to the language of the tweet, the length of the tweet, the contents of the tweet, the number of followers 
the individual had, and what timeframe will be analysed to determine the most appropriate and accurate result.

###2. __Describe two approaches using data analytics to predict whether a tweet will go viral or not.__
Jenders et.al (2013) states that there are multiple variables to consider when determining whether a tweet will go viral or not. Through 
experiments they were able to recognise a strong relationship with the number of followers and average number of retweets, and the 
length of tweet and average number of retweets.

1.	The first approach is derived from the Generalised Linear Model (GLM) described by Jenders et.al (2013). This approach determines 
the probability that the number of retweets that a specific tweet will receive (i.e. the probability that the tweet will go viral) above 
the pre-defined threshold. The model will train on a given set of individual tweets which are randomly split from the dataset, as well 
as their associated features which include the number of followers and length of the tweets related to the average number of retweets. 
These features are then given a weighting, and together with each tweet’s vector, a score is created to determine the potential the 
tweet has of going viral. Further, the “viral score” can be applied with a generalised sigmoid activation function to produce the 
probability score.

2. The second approach is derived from Marukatat’s (2016) approach of using a Naïve Bayes classifier. This classifier’s prediction is 
calculated based off the probability of each of the classes based on the selected attributes in a new case. These attributes are stated 
as being independent from one another and the probability is calculated through the randomised training dataset. Although the results 
shown in the tests performed by Marukatat (2016) display other classifiers such as the decision-tree, have a higher accuracy percentage, 
the Naïve Bayes classifier has a greater capability to handle larger datasets, which is ideal for this problem. The accuracies from the 
Naive Bayes classifier presented by Marukatat (2016) show an average of 84% accuracy across all the tests.

###3. __How would you validate these approaches?__
The most logical and appropriate way to validate these approaches would be to execute the approaches using the same datasets (training 
and test). There are a few methods that can be used to determine the performance of the selected approach, such as; the Holdout method, 
Cross-validation method and the Bootstrap method (Linkopling University, 2016).

####Holdout Method
The Holdout method splits the data into two sets of data (the training and testing datasets). The most common split for this method is 
70% training, 30% testing, however if the split is not randomised, the data samples may not be an accurate representation of the 
approach. This method only produces an estimate of the classifier/approach and it can become more reliable if the method is repeated 
with various subsamples of training and testing data. This can be achieved in one of two ways; setting the seed to a specific number so 
that the data is split the same way either time and the end user must manually change the number to receive different results. The 
alternative way is to use a random seed which eiminates the manual input and potential bias within the data split. 

####Cross-validation Method
The Cross-validation method avoids overlapping test sets and in most cases the datasets are put through the stratification method before 
the cross-validation method. Previous tests have shown that the best performing method for evaluation is the "stratified ten-fold cross-
validation method" (Linkopling University, 2016). This method produces both an acurracy and error rate (which is derived from the 
average to yield the overall error estimate).

####Bootstrap Method
Unlike the Cross-validation method, the Bootstrap method uses a sampling method that replaces data to from the training set. A sample of 
the dataset that is used for training the model consists of *n* instances *n* times while including replacement techniques to form the 
new dataset of *n* instances. The instances that do not occur in the training dataset are used in the testing dataset. This method uses 
a combination of the error estimate and the resubstitution error to produce an error rate.

Therefore at least one of these methods should be used to validate the two approaches. In an ideal world, all three methods, as well as 
other alternative methods should be used to evaluate the performance of each approach. In using multiple methods to evaluate the 
approaches, they can ultimately be improved to produce the best possible solution to the problem at hand.

###4. __Discuss the ethical and social consequences of this study__
When collecting data from Twitter there are several ethical and social consequences that may arise from this study. The first potential 
consequence would be that personal information about the individual could be collected in the data retrieval process. In the perfect 
scenario, any personal information would not be collected, and the tweets would be anonymised. However, quite often individuals have 
multiple social media accounts connected to each other and information could be collected unknowingly throughout the data collection 
stage. This additional collected information could de-anonymise the individual and their tweets, therefore making their personal 
information readily available to others without them knowing. This example raises the concern of privacy around the process of 
retrieving these tweets from individuals, as either approach could be used to exploit the data and information collected from the user.

It is also difficult to monitor and regulate the data and information that is being collected by individuals, groups and third-party 
organisations that use social media applications for marketing purposes. This raises the question as to whether these approaches are 
ethically and socially acceptable and appropriate, as there can be several consequences for individuals if their data and information 
is misused, including identity theft and misuse of personal data for company gains or profits.

###References
Jenders, M., Kasneci, G. & Naumann, F. 2013, 'Analyzing and Predicting Viral Tweets', *Proceedings of the 22nd International Conference 
on World Wide Web*, pp. 657-664.
Linkopling University, 2016, Data Mining Evaluation of a Classifier, *Subject TMN033: Introduction to Data Mining*, lecture notes, 
Sweden, viewed 9 October 2019, <http://staffwww.itn.liu.se/~aidvi/courses/06/dm/lectures/lec6.pdf>.
Karsligil, O. 2018, *What are ethical issues in data mining?*, Quora, viewed 7 October 2019, <https://www.quora.com/What-are-ethical-
issues-in-data-mining>.
Sayce, D. 2019, *Number of tweets per day?*, David Sayce, viewed 7 October 2019, <https://www.dsayce.com/social-media/tweets-day/>.
Twitter, 2019, *Docs - Twitter Developers*, Twitter, viewed 6 October 2019, <https://developer.twitter.com/en/docs>.

###Appendix
GitHub Respository: https://github.com/jmanning21/31005_MachineLearning_Python/blob/master/Assignment_3_12875795.ipynb
Plain text: https://github.com/jmanning21/31005_MachineLearning_Python/blob/master/Assignment_3_12875795_PLAIN.txt
