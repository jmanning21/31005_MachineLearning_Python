{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmanning21/31005_MachineLearning_Python/blob/master/Machine_Learning_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxDh23ZzhcyU",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 2: Practical Machine Learning Project\n",
        "####Affnan Amir (13528841) and Julia Manning (12875795)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjezS5t_-Gzl",
        "colab_type": "text"
      },
      "source": [
        "###Introduction\n",
        "In this assignment there were many classifiers developed during the process of analysing the dataset using Python. They were many different classifiers we have used but the algorithm chosen for this project is the ID3 Decision Tree building algorithm. The chosen input dataset is *titanic.csv* (which can be accessed through the URL provided in the Appendix). The problem is that whether the chosen classifier can predict the survival rate of individuals using the given dataset. A binary decision tree is a structure based on a sequential decision process. Starting from the root, a feature is evaluated, and one of the two branches is selected. This procedure is repeated until a final leaf is reached, which normally represents the classification target. This will be shown in our chosen data set predicting the survivors of the titanic ship based on a binary decision tree method. The Decision trees seem to be simpler in their dynamics, however, if the dataset is able to be split, while keeping an internal balance, the overall process is intuitive and rather fast in its predictions. Using the decision tree is powerful prediction to determine accurately and is also commonly used in algorithms as a classifier. Decision tree can explain exactly why a specific prediction was made making it a good use for operational use.\n",
        "\n",
        "* Output = predicted results for those individuals which survived (i.e. binary options; Did they survive? Yes or No)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqiTYMTfEyE7",
        "colab_type": "text"
      },
      "source": [
        "###Exploration\n",
        "\n",
        "An information structure is a specific method for sorting out information in a PC with the goal that it tends to be utilized adequately. Bits of the raw data had to be taken out such as different attributes and their instances which were inconsistent. For example, some formats had to be adjusted as they werenâ€™t consistent throughout the dataset and had to be accustomed to the correct format. This is the why that the data was cleaned so that we could progress to the code. \n",
        "\n",
        "For some cases I was altering between integers and strings and moving back and forth to strings and integers for particular attributes. Some other problems with the data mining was some bits of the data was poor quality, inadequate data size and poor illustration of data sampling. Dealing with very big datasets that require distributed approaches was also a problem when data was mined. Finally, what else I thought was difficult was that the processing of large, complex and unstructured data and placing it into a structured format.\n",
        "\n",
        "Data pre-processing is a data mining technique which requires transforming data that is raw into a readable and understandable format, data pre-processing is essential before we even start mining the data to find any real-world solutions from the raw data. It is also essential because it leads to an accurate prediction of different classifications and models. Big data or real-world data is often unstructured or incomplete, different trends, and sometime may be inconsistent. Also, we can see that there is a lacking attribute value, which are the reason why the data becomes noisy meaning that it contains errors or outliers. Data pre-processing solves these problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5b_yI15si8f",
        "colab_type": "text"
      },
      "source": [
        "###Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgKARsRudDjs",
        "colab_type": "text"
      },
      "source": [
        "There are a few vital steps that need to be achieved in order for the ID3 Tree building algorithm to be achieved. \n",
        "\n",
        "1.The first step required is the importing of the required packages, which include; numpy, pandas, and sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN1ZCy7rspT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOn6Ue4Wd_uw",
        "colab_type": "text"
      },
      "source": [
        "2.The data must be imported and transformed from a csv format into a readable dataframe using the pandas library. \n",
        "\n",
        "\n",
        "3.The required transformations must then be applied to the dataset, in order to adjust the classifier to the given dataset. For this dataset, there were some columns that the needed to be removed and added, as well as replacing NULL values with given median values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPfS2i6js174",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function importing dataset and cleaning\n",
        "def importdata(): \n",
        "  url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "  # Read the csv and convert to a dataframe\n",
        "  d = pd.read_csv(url)\n",
        "  df = pd.DataFrame(data=d)\n",
        "\n",
        "  median_values = df.groupby(['Pclass','Sex'])['Age'].transform('median')\n",
        "  df['Age'].fillna(median_values, inplace=True)\n",
        "  \n",
        "  # Adding and removing specific columns which may help with the accuracy of the accuracy.\n",
        "  # Add a Child.Adult column based on the age of the individual\n",
        "  df['Child.Adult'] = 'Adult'\n",
        "\n",
        "  # Child = Age < 13 and Adult = Age >= 13 \n",
        "  df['Child.Adult'][df['Age'] < 13.0] = 'Child'\n",
        "\n",
        "  # Remove 'Name' column as it does not provide any relevant information to the dataset, as all the values are unique\n",
        "  del df['Name']\n",
        "  # Remove 'Cabin' as there are too many NA values and the remaining values are all unique, make it impossible to determine a suitable replacement for the NA values\n",
        "  del df['Cabin']\n",
        "\n",
        "  # Re-ordering the columns to put the 'Survived' column last\n",
        "  df = df[['PassengerId','Pclass','Sex','Age','SibSp','Parch','Ticket','Fare','Embarked','Child.Adult','Survived']]\n",
        "  \n",
        "  df['Sex'],_ = pd.factorize(df['Sex'])\n",
        "  df['Embarked'],_ = pd.factorize(df['Embarked'])\n",
        "  df['Child.Adult'],_ = pd.factorize(df['Child.Adult'])\n",
        "  df['Ticket'],_ = pd.factorize(df['Ticket'])\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGovFkPihuZk",
        "colab_type": "text"
      },
      "source": [
        "4.The next step required is to split the data based on the user's input value and recognise what the target variable is. The split was not set as a static value for the purposes of testing and gives the user freedom to interact with the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYcfqxjGtSPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to split the data\n",
        "def splitdataset(df,split):\n",
        "  # Seperating the target variable \n",
        "    X = df.iloc[:,:-1]\n",
        "    Y = df.iloc[:,-1] \n",
        "    test_split = 1 - split\n",
        "    \n",
        "    # Spliting the dataset into train and test \n",
        "    X_train, X_test, y_train, y_test = train_test_split(  \n",
        "    X, Y, test_size = test_split, random_state = 100) \n",
        "    \n",
        "    return X, Y, X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK2GAsWbjkVC",
        "colab_type": "text"
      },
      "source": [
        "5.The next step is to create the classifier objects. For this classifier model, two classifier objects have been produced using the Gini Index and the Entropy Index. This is used to compare the two and determine which is the most appropriat eto use for the classifier and the given dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41lq8d5MwICu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to perform training with giniIndex. \n",
        "def train_using_gini(X_train, X_test, y_train): \n",
        "  \n",
        "    # Creating the classifier object \n",
        "    clf_gini = DecisionTreeClassifier(\n",
        "        criterion = \"gini\", random_state = 0,\n",
        "        max_depth=3, min_samples_leaf=5) \n",
        "  \n",
        "    # Performing training \n",
        "    clf_gini.fit(X_train, y_train) \n",
        "    return clf_gini "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcBy3wBnwIBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to perform training with entropy. \n",
        "def train_using_entropy(X_train, X_test, y_train): \n",
        "  \n",
        "    # Decision tree with entropy \n",
        "    clf_entropy = DecisionTreeClassifier( \n",
        "            criterion = \"entropy\", random_state = 0, \n",
        "            max_depth = 3, min_samples_leaf = 5) \n",
        "  \n",
        "    # Performing training \n",
        "    clf_entropy.fit(X_train, y_train) \n",
        "    return clf_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUB60GLYwIAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to make predictions \n",
        "def prediction(X_test, clf_object): \n",
        "  \n",
        "    # Predicton on test data\n",
        "    y_pred = clf_object.predict(X_test) \n",
        "    print(\"Predicted values:\") \n",
        "    print(y_pred) \n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIw34vaHwd-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate accuracy \n",
        "def cal_accuracy(y_test, y_pred): \n",
        "      \n",
        "    print(\"Confusion Matrix: \", \n",
        "        confusion_matrix(y_test, y_pred)) \n",
        "      \n",
        "    print (\"Accuracy : {:.2f}%\".format(accuracy_score(y_test,y_pred)*100)) \n",
        "            \n",
        "    print(\"Report : \", \n",
        "    classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9XUebg1wntb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Driver code \n",
        "def main(var): \n",
        "      \n",
        "    # Building Phase \n",
        "    data = importdata()\n",
        "    X, Y, X_train, X_test, y_train, y_test = splitdataset(data,var) \n",
        "    clf_gini = train_using_gini(X_train, X_test, y_train) \n",
        "    clf_entropy = train_using_entropy(X_train, X_test, y_train) \n",
        "      \n",
        "    # Operational Phase \n",
        "    print(\"Results Using Gini Index:\") \n",
        "      \n",
        "    # Prediction using gini \n",
        "    y_pred_gini = prediction(X_test, clf_gini) \n",
        "    cal_accuracy(y_test, y_pred_gini) \n",
        "      \n",
        "    print(\"Results Using Entropy:\") \n",
        "    # Prediction using entropy \n",
        "    y_pred_entropy = prediction(X_test, clf_entropy) \n",
        "    cal_accuracy(y_test, y_pred_entropy) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWY99ANawsWS",
        "colab_type": "code",
        "outputId": "4be1735d-8af5-4e09-ae95-3c0a2ae54462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Run the entire code\n",
        "while True:\n",
        "  try:\n",
        "    var = float(input(\"Enter the training data split number: \"))\n",
        "    \n",
        "    if var <= 0.99 and var >= 0.01:\n",
        "      main(var) \n",
        "    \n",
        "  except ValueError:\n",
        "    print(\"This process has been stopped.\")\n",
        "    print(\"To continue rerun and enter a number between 0.01 and 0.99\")\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the training data split number: 0.72\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Results Using Gini Index:\n",
            "Predicted values:\n",
            "[1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            "Confusion Matrix:  [[131  16]\n",
            " [ 33  70]]\n",
            "Accuracy : 80.40%\n",
            "Report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.89      0.84       147\n",
            "           1       0.81      0.68      0.74       103\n",
            "\n",
            "    accuracy                           0.80       250\n",
            "   macro avg       0.81      0.79      0.79       250\n",
            "weighted avg       0.81      0.80      0.80       250\n",
            "\n",
            "Results Using Entropy:\n",
            "Predicted values:\n",
            "[1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            "Confusion Matrix:  [[131  16]\n",
            " [ 32  71]]\n",
            "Accuracy : 80.80%\n",
            "Report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.89      0.85       147\n",
            "           1       0.82      0.69      0.75       103\n",
            "\n",
            "    accuracy                           0.81       250\n",
            "   macro avg       0.81      0.79      0.80       250\n",
            "weighted avg       0.81      0.81      0.80       250\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-2b35228cee3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the training data split number: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.99\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQuWKfGWE5RN",
        "colab_type": "text"
      },
      "source": [
        "###Evaluation\n",
        "Improvements for future renditions: \n",
        "- When producing the ages for null values in the dataset, the ages could have been grouped by class and gender and class of the individual, rather than just using gender. This may produce a more accurate number. \n",
        "- Use Gini Idex instead of Entropy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHQsYEjgDbE2",
        "colab_type": "text"
      },
      "source": [
        "###Results\n",
        "This is the results from the decision tree model, displaying the accuracy outcomes for the changing variables\n",
        "\n",
        "Train Data Split|Criterion|Max Depth|Misclassified|Accuracy\n",
        "---|---|---|---|---\n",
        "0.6|Entropy|3|70|0.8039\n",
        "0.6|Entropy|5|68|0.8095\n",
        "0.6|Gini|3|67|0.8123\n",
        "0.6|Gini|5|77|0.7843\n",
        "0.7|Entropy|3|48|0.8209\n",
        "0.7|Entropy|5|48|0.8209\n",
        "0.7|Gini|3|48|0.8209\n",
        "0.7|Gini|5|49|0.8172"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb1GeQENE8Jj",
        "colab_type": "text"
      },
      "source": [
        "###Conclusion\n",
        "In this Assignment we had designed different models using python and make the predictions based of survival rate of the sinking titanic ship and the accuracy shows that it was approximately 81%, which is decent and accurate. The possible improvements can be finding the intermediate by implementing three different strategies including; oversampling, under-sampling, and a combined approach. Testing the initial hypothesis is also a strategy improves the accuracy towards the end.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoVVVg29E-B2",
        "colab_type": "text"
      },
      "source": [
        "###Ethical Justification\n",
        "\n",
        "In practice, we are trying to create a set of numerous black boxes and choose the one with the highest precision. We need a lot of data and an algorithm to build these. From this information, the algorithm learns. Each black box in a set has the same algorithm in a slightly distinct variant. Then we choose the most precise version. Machine learning algorithms are now finding their way into countless apps, some of which have a major effect on human society. This is a classic statistical problem. If you have a big quantity of information and then compare one set of information to another, you may discover instances that appear to be connected together. Not only are machine learning algorithms far from ideal, they come with no estimate of their predictions ' uncertainty. This can be shown as our accuracy is 82 percent but there are many outliers. The title implies another issue with machine learning. It's machine learning. It is not always the same way that humans and machines \"believe.\" I will look briefly at the perception issue to demonstrate this. In the field of machine vision, this is essential in which a robot views its environment and makes choices about what it sees. The apporach that was used, was the utalitarian approach. this assesses an action in terms of its consequences or outcomes. Utilitarianism is a normative ethical concept which only places the locus of right and incorrect on the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5QikhaZFA0x",
        "colab_type": "text"
      },
      "source": [
        "###Video Pitch\n",
        "Link to video pitch via YouTube:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN-SK2iHGT6L",
        "colab_type": "text"
      },
      "source": [
        "###Appendix \n",
        "- Input dataset URL: https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\n",
        "- GitHub URL: \n",
        "https://github.com/jmanning21/31005_MachineLearning_Python/blob/master/Machine_Learning_Assignment_2.ipynb\n",
        "- GitHub PLAIN.text URL:"
      ]
    }
  ]
}