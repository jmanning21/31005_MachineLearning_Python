{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "U5b_yI15si8f"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmanning21/31005_MachineLearning_Python/blob/master/Machine_Learning_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxDh23ZzhcyU",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 2: Practical Machine Learning Project\n",
        "####Affnan Amir (13528841) and Julia Manning (12875795)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjezS5t_-Gzl",
        "colab_type": "text"
      },
      "source": [
        "###Introduction\n",
        "In this assignment there were many classifiers developed during the process of analysing the dataset using Python. They were many different classifiers we have used but the algorithm chosen for this project is the ID3 Decision Tree building algorithm. The chosen input dataset is *titanic.csv* (which can be accessed through the URL provided in the Appendix). The problem is that whether the chosen classifier can predict the survival rate of individuals using the given dataset. A binary decision tree is a structure based on a sequential decision process. Starting from the root, a feature is evaluated, and one of the two branches is selected. This procedure is repeated until a final leaf is reached, which normally represents the classification target. The output which will be produced from this classifier includes the following from using both the Gini Index and the Entropy Index; an array of the predicted values, confusion matrix, accuracy score, and a classification report. The Decision trees seem to be simpler in their dynamics, however, if the dataset is able to be split, while keeping an internal balance, the overall process is intuitive and rather fast in its predictions. Using the decision tree is powerful prediction to determine accurately and is also commonly used in algorithms as a classifier. Decision tree can explain exactly why a specific prediction was made making it a good use for operational use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqiTYMTfEyE7",
        "colab_type": "text"
      },
      "source": [
        "###Exploration\n",
        "\n",
        "An information structure is a specific method for sorting out information in a PC with the goal that it tends to be utilized adequately. Bits of the raw data had to be taken out such as different attributes and their instances which were inconsistent. For example, some formats had to be adjusted as they werenâ€™t consistent throughout the dataset and had to be accustomed to the correct format. This is the why that the data was cleaned so that we could progress to the code. \n",
        "\n",
        "For some cases I was altering between integers and strings and moving back and forth to strings and integers for particular attributes. Some other problems with the data mining was some bits of the data was poor quality, inadequate data size and poor illustration of data sampling. Dealing with very big datasets that require distributed approaches was also a problem when data was mined. Finally, what else I thought was difficult was that the processing of large, complex and unstructured data and placing it into a structured format.\n",
        "\n",
        "Data pre-processing is a data mining technique which requires transforming data that is raw into a readable and understandable format, data pre-processing is essential before we even start mining the data to find any real-world solutions from the raw data. It is also essential because it leads to an accurate prediction of different classifications and models. Big data or real-world data is often unstructured or incomplete, different trends, and sometime may be inconsistent. Also, we can see that there is a lacking attribute value, which are the reason why the data becomes noisy meaning that it contains errors or outliers. Data pre-processing solves these problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5b_yI15si8f",
        "colab_type": "text"
      },
      "source": [
        "###Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgKARsRudDjs",
        "colab_type": "text"
      },
      "source": [
        "There are vital steps that need to be achieved in order for the ID3 Tree building algorithm to be achieved. The steps are explained below and are followed by the relevant code snippets. \n",
        "\n",
        "1.The first step required is the importing of the required packages, which include; numpy, pandas, and sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN1ZCy7rspT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOn6Ue4Wd_uw",
        "colab_type": "text"
      },
      "source": [
        "2.The data must be imported and transformed from a csv format into a readable dataframe using the pandas library. \n",
        "\n",
        "\n",
        "3.The required transformations must then be applied to the dataset, in order to adjust the classifier to the given dataset. For this dataset, there were some columns that the needed to be removed and added, as well as replacing NULL values with given median values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPfS2i6js174",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function importing dataset and cleaning\n",
        "def importdata(): \n",
        "  url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "  # Read the csv and convert to a dataframe\n",
        "  d = pd.read_csv(url)\n",
        "  df = pd.DataFrame(data=d)\n",
        "\n",
        "  median_values = df.groupby(['Pclass','Sex'])['Age'].transform('median')\n",
        "  df['Age'].fillna(median_values, inplace=True)\n",
        "  \n",
        "  # Adding and removing specific columns which may help with the accuracy of the accuracy.\n",
        "  # Add a Child.Adult column based on the age of the individual\n",
        "  df['Child.Adult'] = 'Adult'\n",
        "\n",
        "  # Child = Age < 13 and Adult = Age >= 13 \n",
        "  df['Child.Adult'][df['Age'] < 13.0] = 'Child'\n",
        "\n",
        "  # Remove 'Name' column as it does not provide any relevant information to the dataset, as all the values are unique\n",
        "  del df['Name']\n",
        "  # Remove 'Cabin' as there are too many NA values and the remaining values are all unique, make it impossible to determine a suitable replacement for the NA values\n",
        "  del df['Cabin']\n",
        "\n",
        "  # Re-ordering the columns to put the 'Survived' column last\n",
        "  df = df[['PassengerId','Pclass','Sex','Age','SibSp','Parch','Ticket','Fare','Embarked','Child.Adult','Survived']]\n",
        "  \n",
        "  df['Sex'],_ = pd.factorize(df['Sex'])\n",
        "  df['Embarked'],_ = pd.factorize(df['Embarked'])\n",
        "  df['Child.Adult'],_ = pd.factorize(df['Child.Adult'])\n",
        "  df['Ticket'],_ = pd.factorize(df['Ticket'])\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGovFkPihuZk",
        "colab_type": "text"
      },
      "source": [
        "4.The next step required is to split the data based on the user's input value and recognise what the target variable is. The split was not set as a static value for the purposes of testing and gives the user freedom to interact with the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYcfqxjGtSPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to split the data\n",
        "def splitdataset(df,split):\n",
        "  # Seperating the target variable \n",
        "    X = df.iloc[:,:-1]\n",
        "    Y = df.iloc[:,-1] \n",
        "    test_split = 1 - split\n",
        "    \n",
        "    # Spliting the dataset into train and test \n",
        "    X_train, X_test, y_train, y_test = train_test_split(  \n",
        "    X, Y, test_size = test_split, random_state = 0) \n",
        "    \n",
        "    return X, Y, X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK2GAsWbjkVC",
        "colab_type": "text"
      },
      "source": [
        "5.The next step is to create the classifier objects. For this classifier model, two classifier objects have been produced using the Gini Index and the Entropy Index. This is used to compare the two and determine which is the most appropriat eto use for the classifier and the given dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41lq8d5MwICu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to perform training with giniIndex. \n",
        "def train_using_gini(X_train, X_test, y_train): \n",
        "  \n",
        "    # Creating the classifier object \n",
        "    clf_gini = DecisionTreeClassifier(\n",
        "        criterion = \"gini\", random_state = 0,\n",
        "        max_depth=3, min_samples_leaf=5) \n",
        "  \n",
        "    # Performing training \n",
        "    clf_gini.fit(X_train, y_train) \n",
        "    return clf_gini "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcBy3wBnwIBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to perform training with entropy. \n",
        "def train_using_entropy(X_train, X_test, y_train): \n",
        "  \n",
        "    # Decision tree with entropy \n",
        "    clf_entropy = DecisionTreeClassifier( \n",
        "            criterion = \"entropy\", random_state = 0, \n",
        "            max_depth = 5, min_samples_leaf = 5) \n",
        "  \n",
        "    # Performing training \n",
        "    clf_entropy.fit(X_train, y_train) \n",
        "    return clf_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it_CnJgZpe78",
        "colab_type": "text"
      },
      "source": [
        "6.The next step is to write a function that predicts the values. This function also prints and returns the predicted values to the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUB60GLYwIAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to make predictions \n",
        "def prediction(X_test, clf_object): \n",
        "  \n",
        "    # Predicton on test data\n",
        "    y_pred = clf_object.predict(X_test) \n",
        "    print(\"Predicted values:\") \n",
        "    print(y_pred) \n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atKw1X98p0qq",
        "colab_type": "text"
      },
      "source": [
        "7.The next step is to calculate the accuracy of the model. This is achieved by using the output from the confusion matrix, which produces a 2-by-2 table of the instances where the predictions are correct and incorrect. The confusion matrix gives the user the four values: True Positive, False Positive, True Negative, and False Negative. A report is also produced, which prints the scores for; precision, recall, F1 and support."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIw34vaHwd-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate accuracy \n",
        "def cal_accuracy(y_test, y_pred): \n",
        "      \n",
        "    print(\"Confusion Matrix: \", \n",
        "        confusion_matrix(y_test, y_pred)) \n",
        "      \n",
        "    print (\"Accuracy : {:.2f}%\".format(accuracy_score(y_test,y_pred)*100)) \n",
        "            \n",
        "    print(\"Report : \", \n",
        "    classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xq5LroerAHW",
        "colab_type": "text"
      },
      "source": [
        "7.One of the final steps is to create a function that runs all the previously executed functions in the correct format and order. This ensures that the model is achieving the required output based on the selected dataset and variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9XUebg1wntb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Driver code \n",
        "def main(var): \n",
        "      \n",
        "    # Building Phase \n",
        "    data = importdata()\n",
        "    X, Y, X_train, X_test, y_train, y_test = splitdataset(data,var) \n",
        "    clf_gini = train_using_gini(X_train, X_test, y_train) \n",
        "    clf_entropy = train_using_entropy(X_train, X_test, y_train) \n",
        "      \n",
        "    # Operational Phase \n",
        "    print(\"Results Using Gini Index:\") \n",
        "      \n",
        "    # Prediction using gini \n",
        "    y_pred_gini = prediction(X_test, clf_gini) \n",
        "    cal_accuracy(y_test, y_pred_gini) \n",
        "      \n",
        "    print(\"Results Using Entropy:\") \n",
        "    # Prediction using entropy \n",
        "    y_pred_entropy = prediction(X_test, clf_entropy) \n",
        "    cal_accuracy(y_test, y_pred_entropy) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otb-18heslBb",
        "colab_type": "text"
      },
      "source": [
        "8.The last step is to allow the user to enter their chosen number for splitting the dat into a training set and a testing set. Error handling must be used to ensure that the classifier is able to continue. Once the user has entered their chosen training data split number and it meets the criteria, the model will run and produce the output specified previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWY99ANawsWS",
        "colab_type": "code",
        "outputId": "df516368-d5a8-49e6-f741-abd1514d9891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        }
      },
      "source": [
        "# Run the entire code\n",
        "while True:\n",
        "  try:\n",
        "    var = float(input(\"Enter the training data split number: \"))\n",
        "    \n",
        "    if var <= 0.99 and var >= 0.01:\n",
        "      main(var) \n",
        "      \n",
        "    else:\n",
        "      print(\"Please enter a number between 0.01 and 0.99\")\n",
        "    \n",
        "  except ValueError:\n",
        "    print(\"This process has been stopped.\")\n",
        "    print(\"To continue rerun and enter a number between 0.01 and 0.99\")\n",
        "    break"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the training data split number: 0\n",
            "Please enter a number between 0.01 and 0.99\n",
            "Enter the training data split number: 0.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Results Using Gini Index:\n",
            "Predicted values:\n",
            "[0 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 0]\n",
            "Confusion Matrix:  [[146  22]\n",
            " [ 25  75]]\n",
            "Accuracy : 82.46%\n",
            "Report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       168\n",
            "           1       0.77      0.75      0.76       100\n",
            "\n",
            "    accuracy                           0.82       268\n",
            "   macro avg       0.81      0.81      0.81       268\n",
            "weighted avg       0.82      0.82      0.82       268\n",
            "\n",
            "Results Using Entropy:\n",
            "Predicted values:\n",
            "[0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 0]\n",
            "Confusion Matrix:  [[152  16]\n",
            " [ 33  67]]\n",
            "Accuracy : 81.72%\n",
            "Report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       168\n",
            "           1       0.81      0.67      0.73       100\n",
            "\n",
            "    accuracy                           0.82       268\n",
            "   macro avg       0.81      0.79      0.80       268\n",
            "weighted avg       0.82      0.82      0.81       268\n",
            "\n",
            "Enter the training data split number: stop\n",
            "This process has been stopped.\n",
            "To continue rerun and enter a number between 0.01 and 0.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQuWKfGWE5RN",
        "colab_type": "text"
      },
      "source": [
        "###Evaluation\n",
        "Improvements for future renditions: \n",
        "- When producing the ages for null values in the dataset, the ages could have been grouped by class and gender and class of the individual, rather than just using gender. This may produce a more accurate number. \n",
        "- Use Gini Idex instead of Entropy?\n",
        "\n",
        "\n",
        "####Results\n",
        "This is the results from the decision tree model, displaying the accuracy outcomes for the changing variables. Only one variable was changed for each test to ensure that the test was valid.\n",
        "\n",
        "Train Data Split (%)|Criterion|Max Depth|Misclassified|Accuracy\n",
        "---|---|---|---|---\n",
        "60|Entropy|3|70|0.8039\n",
        "60|Entropy|5|68|0.8095\n",
        "60|Gini|3|67|0.8123\n",
        "60|Gini|5|77|0.7843\n",
        "70|Entropy|3|48|0.8209\n",
        "70|Entropy|5|48|0.8246\n",
        "70|Gini|3|48|0.8209\n",
        "70|Gini|5|49|0.8172"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb1GeQENE8Jj",
        "colab_type": "text"
      },
      "source": [
        "###Conclusion\n",
        "In this assignment we compared two classifer objects: Gini Index and entropy Index, within the ID3 Decision Tree building alogrithm model. This was implemented from scratch using Python, and produced an output that identified if the individuals in the test dataset would survive for both classifier objects. The classifier was tested by adusting a few variables, to help determine which parameters produced a higher accuracy rate. The highest accuracy achieved was 82.46%, which was achieved using a 70/30 data split and the Entropy index. For future implementations, a larger dataset should be used, as ID3 Decision Tree algorithms often over-fit data, which can cause a bias in the output. Additional columns should also be used to help categorise the columns that were removed (i.e. \"Name\" and \"Cabin\"), as this may help with the overall accuracy of the classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoVVVg29E-B2",
        "colab_type": "text"
      },
      "source": [
        "###Ethical Justification\n",
        "\n",
        "In practice, we are trying to create a set of numerous black boxes and choose the one with the highest precision. We need a lot of data and an algorithm to build these. From this information, the algorithm learns. Each black box in a set has the same algorithm in a slightly distinct variant. Then we choose the most precise version. Machine learning algorithms are now finding their way into countless apps, some of which have a major effect on human society. This is a classic statistical problem. If you have a big quantity of information and then compare one set of information to another, you may discover instances that appear to be connected together. Not only are machine learning algorithms far from ideal, they come with no estimate of their predictions ' uncertainty. This can be shown as our accuracy is 82% but there are many outliers. The title implies another issue with machine learning. It's machine learning. It is not always the same way that humans and machines \"believe.\" I will look briefly at the perception issue to demonstrate this. In the field of machine vision, this is essential in which a robot views its environment and makes choices about what it sees. The apporach that was used, was the utalitarian approach, as this assesses an action in terms of its consequences or outcomes. Utilitarianism is a normative ethical concept which only places the locus of right and incorrect on the results. A judgement of the potential negative impact of misusing this classifier was made, and it was determined that the risk and impact would be significantly low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5QikhaZFA0x",
        "colab_type": "text"
      },
      "source": [
        "###Video Pitch\n",
        "Link to video pitch via YouTube: https://youtu.be/Sbe6KTzjJPg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN-SK2iHGT6L",
        "colab_type": "text"
      },
      "source": [
        "###Appendix \n",
        "- Input dataset URL: https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\n",
        "- GitHub URL: \n",
        "https://github.com/jmanning21/31005_MachineLearning_Python/blob/master/Machine_Learning_Assignment_2.ipynb\n",
        "- GitHub PLAIN.text URL:"
      ]
    }
  ]
}