{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmanning21/31005_MachineLearning_Python/blob/master/Machine_Learning_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxDh23ZzhcyU",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 2: Practical Machine Learning Project\n",
        "####Affnan Amir (13528841) and Julia Manning (12875795)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjezS5t_-Gzl",
        "colab_type": "text"
      },
      "source": [
        "###Introduction\n",
        "In this assignment there were many classifiers developed during the process of analysing the dataset using Python. They were many different classifiers we have used but the algorithm chosen for this project is the ID3 Decision Tree building algorithm. The chosen input dataset is *titanic.csv* (which can be accessed through the URL provided in the Appendix). The problem is that whether the chosen classifier can predict the survival rate of individuals using the given dataset. A binary decision tree is a structure based on a sequential decision process. Starting from the root, a feature is evaluated, and one of the two branches is selected. This procedure is repeated until a final leaf is reached, which normally represents the classification target. The output which will be produced from this classifier includes the following from using both the Gini Index and the Entropy Index; an array of the predicted values, confusion matrix, accuracy score, and a classification report. The Decision trees seem to be simpler in their dynamics, however, if the dataset is able to be split, while keeping an internal balance, the overall process is intuitive and rather fast in its predictions. Using the decision tree is powerful prediction to determine accurately and is also commonly used in algorithms as a classifier. Decision tree can explain exactly why a specific prediction was made making it a good use for operational use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqiTYMTfEyE7",
        "colab_type": "text"
      },
      "source": [
        "###Exploration\n",
        "\n",
        "A data structure is a particular technique for dealing with data in a PC with the objective that it will in general be used satisfactorily. Bits of the crude information must be taken out, for example, various properties and their occasions which were conflicting. For instance, a few arrangements must be balanced as they weren't predictable all through the dataset and must be familiar with the right position. This is the why that the information was cleaned with the goal that we could advance to the code. \n",
        "\n",
        "For certain cases I was adjusting among whole numbers and strings and moving to and fro to strings and whole numbers for specific qualities. Some different issues with the information mining was a few bits of the information was low quality, lacking information size and poor representation of information testing. Managing exceptionally huge datasets that require appropriated methodologies was likewise an issue when information was mined. At long last, what else I thought was troublesome was that the handling of huge, mind boggling and unstructured information and putting it into an organized configuration. \n",
        "\n",
        "Information pre-handling is an information mining procedure which requires changing information that is crude into a coherent and reasonable organization, information pre-preparing is basic before we even start mining the information to locate any certifiable arrangements from the crude information. It is likewise fundamental since it prompts a precise forecast of various arrangements and models. Huge information or genuine information is frequently unstructured or deficient, various patterns, and at some point might be conflicting. Likewise, we can see that there is a lacking property estimation, which are the motivation behind why the information winds up boisterous implying that it contains blunders or exceptions. Information pre-handling takes care of these issue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5b_yI15si8f",
        "colab_type": "text"
      },
      "source": [
        "###Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgKARsRudDjs",
        "colab_type": "text"
      },
      "source": [
        "There are vital steps that need to be achieved in order for the ID3 Tree building algorithm to be achieved. The steps are explained below and are followed by the relevant code snippets. \n",
        "\n",
        "1.The first step required is the importing of the required packages, which include; numpy, pandas, and sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN1ZCy7rspT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOn6Ue4Wd_uw",
        "colab_type": "text"
      },
      "source": [
        "2.The data must be imported and transformed from a csv format into a readable dataframe using the pandas library. \n",
        "\n",
        "\n",
        "3.The required transformations must then be applied to the dataset, in order to adjust the classifier to the given dataset. For this dataset, there were some columns that the needed to be removed and added, as well as replacing NULL values with given median values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPfS2i6js174",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function importing dataset and cleaning\n",
        "def importdata(): \n",
        "  url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "  # Read the csv and convert to a dataframe\n",
        "  d = pd.read_csv(url)\n",
        "  df = pd.DataFrame(data=d)\n",
        "\n",
        "  median_values = df.groupby(['Pclass','Sex'])['Age'].transform('median')\n",
        "  df['Age'].fillna(median_values, inplace=True)\n",
        "  \n",
        "  # Adding and removing specific columns which may help with the accuracy of the accuracy.\n",
        "  # Add a Child.Adult column based on the age of the individual\n",
        "  df['Child.Adult'] = 'Adult'\n",
        "\n",
        "  # Child = Age < 13 and Adult = Age >= 13 \n",
        "  df['Child.Adult'][df['Age'] < 13.0] = 'Child'\n",
        "\n",
        "  # Remove 'Name' column as it does not provide any relevant information to the dataset, as all the values are unique\n",
        "  del df['Name']\n",
        "  # Remove 'Cabin' as there are too many NA values and the remaining values are all unique, make it impossible to determine a suitable replacement for the NA values\n",
        "  del df['Cabin']\n",
        "\n",
        "  # Re-ordering the columns to put the 'Survived' column last\n",
        "  df = df[['PassengerId','Pclass','Sex','Age','SibSp','Parch','Ticket','Fare','Embarked','Child.Adult','Survived']]\n",
        "  \n",
        "  df['Sex'],_ = pd.factorize(df['Sex'])\n",
        "  df['Embarked'],_ = pd.factorize(df['Embarked'])\n",
        "  df['Child.Adult'],_ = pd.factorize(df['Child.Adult'])\n",
        "  df['Ticket'],_ = pd.factorize(df['Ticket'])\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGovFkPihuZk",
        "colab_type": "text"
      },
      "source": [
        "4.The next step required is to split the data based on the user's input value and recognise what the target variable is. The split was not set as a static value for the purposes of testing and gives the user freedom to interact with the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYcfqxjGtSPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to split the data\n",
        "def splitdataset(df,split):\n",
        "  # Seperating the target variable \n",
        "    X = df.iloc[:,:-1]\n",
        "    Y = df.iloc[:,-1] \n",
        "    test_split = 1 - split\n",
        "    \n",
        "    # Spliting the dataset into train and test \n",
        "    X_train, X_test, y_train, y_test = train_test_split(  \n",
        "    X, Y, test_size = test_split, random_state = 0) \n",
        "    \n",
        "    return X, Y, X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK2GAsWbjkVC",
        "colab_type": "text"
      },
      "source": [
        "5.The next step is to create the classifier objects. For this classifier model, two classifier objects have been produced using the Gini Index and the Entropy Index. This is used to compare the two and determine which is the most appropriat eto use for the classifier and the given dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41lq8d5MwICu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to perform training with giniIndex. \n",
        "def train_gini(X_train, X_test, y_train): \n",
        "  \n",
        "    # Creating the classifier object \n",
        "    classifier_gini = DecisionTreeClassifier(\n",
        "        criterion = \"gini\", random_state = 0,\n",
        "        max_depth=3, min_samples_leaf=5) \n",
        "  \n",
        "    # Performing training \n",
        "    classifier_gini.fit(X_train, y_train) \n",
        "    return classifier_gini "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcBy3wBnwIBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to perform training with entropy. \n",
        "def train_entropy(X_train, X_test, y_train): \n",
        "  \n",
        "    # Decision tree with entropy \n",
        "    classifier_entropy = DecisionTreeClassifier( \n",
        "            criterion = \"entropy\", random_state = 0, \n",
        "            max_depth = 5, min_samples_leaf = 5) \n",
        "  \n",
        "    # Performing training \n",
        "    classifier_entropy.fit(X_train, y_train) \n",
        "    return classifier_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it_CnJgZpe78",
        "colab_type": "text"
      },
      "source": [
        "6.The next step is to write a function that predicts the values. This function also prints and returns the predicted values to the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUB60GLYwIAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to make predictions \n",
        "def prediction(X_test, classifier_object): \n",
        "  \n",
        "    # Predicton on test data\n",
        "    y_pred = classifier_object.predict(X_test) \n",
        "    print(\"Predicted values:\") \n",
        "    print(y_pred) \n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atKw1X98p0qq",
        "colab_type": "text"
      },
      "source": [
        "7.The next step is to calculate the accuracy of the model. This is achieved by using the output from the confusion matrix, which produces a 2-by-2 table of the instances where the predictions are correct and incorrect. The confusion matrix gives the user the four values: True Positive, False Positive, True Negative, and False Negative. A report is also produced, which prints the scores for; precision, recall, F1 and support."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIw34vaHwd-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate accuracy \n",
        "def cal_accuracy(y_test, y_pred): \n",
        "      \n",
        "    print(\"Confusion Matrix: \", \n",
        "        confusion_matrix(y_test, y_pred)) \n",
        "      \n",
        "    print (\"Accuracy : {:.2f}%\".format(accuracy_score(y_test,y_pred)*100)) \n",
        "            \n",
        "    print(\"Report : \", \n",
        "    classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xq5LroerAHW",
        "colab_type": "text"
      },
      "source": [
        "8.One of the final steps is to create a function that runs all the previously executed functions in the correct format and order. This ensures that the model is achieving the required output based on the selected dataset and variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9XUebg1wntb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main function controlling all the functions\n",
        "def main(var): \n",
        "      \n",
        "    # Building Phase \n",
        "    data = importdata()\n",
        "    X, Y, X_train, X_test, y_train, y_test = splitdataset(data,var) \n",
        "    classifier_gini = train_gini(X_train, X_test, y_train) \n",
        "    classifier_entropy = train_entropy(X_train, X_test, y_train) \n",
        "      \n",
        "    # Operational Phase \n",
        "    print(\"Results Using Gini Index:\") \n",
        "      \n",
        "    # Prediction using gini \n",
        "    y_pred_gini = prediction(X_test, classifier_gini) \n",
        "    cal_accuracy(y_test, y_pred_gini) \n",
        "      \n",
        "    print(\"Results Using Entropy:\") \n",
        "    # Prediction using entropy \n",
        "    y_pred_entropy = prediction(X_test, classifier_entropy) \n",
        "    cal_accuracy(y_test, y_pred_entropy) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otb-18heslBb",
        "colab_type": "text"
      },
      "source": [
        "9.The last step is to allow the user to enter their chosen number for splitting the dat into a training set and a testing set. Error handling must be used to ensure that the classifier is able to continue. Once the user has entered their chosen training data split number and it meets the criteria, the model will run and produce the output specified previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWY99ANawsWS",
        "colab_type": "code",
        "outputId": "fb5a868b-5720-458f-bdfe-05224cf47e11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        }
      },
      "source": [
        "# Run the entire code\n",
        "while True:\n",
        "  try:\n",
        "    var = float(input(\"Enter the training data split number: \"))\n",
        "    \n",
        "    if var <= 0.99 and var >= 0.01:\n",
        "      main(var) \n",
        "      \n",
        "    else:\n",
        "      print(\"Please enter a number between 0.01 and 0.99\")\n",
        "    \n",
        "  except ValueError:\n",
        "    print(\"This process has been stopped.\")\n",
        "    print(\"To continue rerun and enter a number between 0.01 and 0.99\")\n",
        "    break"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the training data split number: 0.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Results Using Gini Index:\n",
            "Predicted values:\n",
            "[0 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 0]\n",
            "Confusion Matrix:  [[146  22]\n",
            " [ 25  75]]\n",
            "Accuracy : 82.46%\n",
            "Report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       168\n",
            "           1       0.77      0.75      0.76       100\n",
            "\n",
            "    accuracy                           0.82       268\n",
            "   macro avg       0.81      0.81      0.81       268\n",
            "weighted avg       0.82      0.82      0.82       268\n",
            "\n",
            "Results Using Entropy:\n",
            "Predicted values:\n",
            "[0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 0]\n",
            "Confusion Matrix:  [[152  16]\n",
            " [ 33  67]]\n",
            "Accuracy : 81.72%\n",
            "Report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       168\n",
            "           1       0.81      0.67      0.73       100\n",
            "\n",
            "    accuracy                           0.82       268\n",
            "   macro avg       0.81      0.79      0.80       268\n",
            "weighted avg       0.82      0.82      0.81       268\n",
            "\n",
            "Enter the training data split number: stop\n",
            "This process has been stopped.\n",
            "To continue rerun and enter a number between 0.01 and 0.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQuWKfGWE5RN",
        "colab_type": "text"
      },
      "source": [
        "###Evaluation\n",
        "The model is written in a succinct manner, involving multiple functions that perform different parts of the required steps to achieve the execution of the ID3 Decision Tree building Algorithm. This allows any other users who wish to understand and use the code for future implementation to easily follow the flow of how the algorithm produces the final output. There is also several comments throughout the algorithm to assist the end user's understanding of what each function is achieving. The produced classifier does not include any loop statements, but rather uses specific built-in functions from existing libraries and packages to execute these tasks in an efficient and timely manner.\n",
        "\n",
        "The efficiency of the algorithm relates to the time and space used when the code is running. The main variables that affect the efficiency of the algorithm relate to simplicity, generality, speed and memory. Various versions have been created in order to reduce the overall run time and memory required to run the classifier. As mentioned before, the simplicity of the code (i.e. in using multiple functions rather than loops) ensures that the algorithm is executed efficiently.\n",
        "\n",
        "####Results from Testing the Algorithm\n",
        "This is the results from the decision tree model, displaying the accuracy outcomes for the changing variables. There were eight (8) tests that were performed on the model to determine whcih parameters produced the highest accuracy percentange. Only one variable was changed for each test to ensure that the test was valid.\n",
        "\n",
        "Train Data Split (%)|Criterion|Max Depth|Misclassified|Accuracy\n",
        "---|---|---|---|---\n",
        "60|Entropy|3|70|0.8039\n",
        "60|Entropy|5|68|0.8095\n",
        "60|Gini|3|67|0.8123\n",
        "60|Gini|5|77|0.7843\n",
        "70|Entropy|3|48|0.8209\n",
        "70|Entropy|5|48|0.8246\n",
        "70|Gini|3|48|0.8209\n",
        "70|Gini|5|49|0.8172\n",
        "\n",
        "From the results table above, it is clear that the best classifier object and parameters for this dataset are:\n",
        "\n",
        "Train Data Split (%)|Criterion|Max Depth|Misclassified|Accuracy\n",
        "---|---|---|---|---\n",
        "70|Entropy|5|48|0.8246\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb1GeQENE8Jj",
        "colab_type": "text"
      },
      "source": [
        "###Conclusion\n",
        "In this assignment we compared two classifer objects: Gini Index and entropy Index, within the ID3 Decision Tree building alogrithm model. This was implemented from scratch using Python, and produced an output that identified if the individuals in the test dataset would survive for both classifier objects. The classifier was tested by adusting a few variables, to help determine which parameters produced a higher accuracy rate. The highest accuracy achieved was 82.46%, which was achieved using a 70/30 data split and the Entropy index. For future implementations, a larger dataset should be used, as ID3 Decision Tree algorithms often over-fit data, which can cause a bias in the output. Additional columns should also be used to help categorise the columns that were removed (i.e. \"Name\" and \"Cabin\"), as this may help with the overall accuracy of the classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoVVVg29E-B2",
        "colab_type": "text"
      },
      "source": [
        "###Ethical Justification\n",
        "\n",
        "The aim of the ID3 Tree Building Algorithm is to produce a model that returns the a high accuracy. When adjusting the parameters to the model, the designated software engineer may produce in-direct bias on the model's output, affecting the overall integrity of the model itself. When testing the model using alternative datasets, this may become a problem if the model has bias tendencies. This could result in unnecessary misclassification, which may have an associated risk factor attached. If the risk factor is high and the individual outcomes are misclassified, this may have detrimental effects on individuals, groups or organisations, depending on what dataset the model is using. In this way, using the Utilitarian Approach would be most appropriate to use, as the model should reflect and benefit the majority of individuals, rather than focusing on individual gain. This approach will aid in reducing the overall potential bias within the given model. To achieve this, it is recommended that multiple different datasets are used when testing the model, and more than one software developer produces the final model. \n",
        "\n",
        "The produced ID3 Tree Building Algorithm above was developed by more than one individual to ensure that the bias was kept to a minimum. A judgement of the potential negative impact of misusing this classifier was made, and it was determined that the risk and impact would be significantly low.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5QikhaZFA0x",
        "colab_type": "text"
      },
      "source": [
        "###Video Pitch\n",
        "Link to video pitch via YouTube: https://youtu.be/Sbe6KTzjJPg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN-SK2iHGT6L",
        "colab_type": "text"
      },
      "source": [
        "###Appendix \n",
        "- Input dataset URL: https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\n",
        "- GitHub URL: \n",
        "https://github.com/jmanning21/31005_MachineLearning_Python/blob/master/Machine_Learning_Assignment_2.ipynb\n",
        "- GitHub PLAIN.text URL: https://github.com/jmanning21/31005_MachineLearning_Python/blob/master/31005_Assignment_2_PLAIN.txt"
      ]
    }
  ]
}