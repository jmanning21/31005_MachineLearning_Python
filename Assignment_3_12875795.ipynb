{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3_12875795",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmanning21/31005_MachineLearning_Python/blob/master/Assignment_3_12875795.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Azdfa3IRm4",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 3: Take Home Exam\n",
        "###Julia Manning | 12875795\n",
        "\n",
        "##Selected Question: Question 3\n",
        "*Marketing or advertising companies would be very interested in being able to predict whether a Twitter message will spread as a meme or not, and even better, construct it so that it will spread. Why is this a hard problem to solve? Describe two approaches using data analytics to predict whether a tweet will go viral or not. How would you validate these approaches? Discuss the ethical and social consequences of this study.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AiTLi4RM72k",
        "colab_type": "text"
      },
      "source": [
        "###1. __Why is this a hard problem to solve?__\n",
        "There are multiple aspects that need to be considered when solving this problem. One of the major problems is retrieving the data. According to Sayce (2019), there are roughly 500 million tweets sent every day, therefore making it difficult to retrieve such a large number of data within a short amount of time, as the life-span of each tweet is relatively short, only lasting 24 minutes before it becomes less relevant. The tweets are also in a non-structured format which makes it more difficult to retrieve the data compared to the traditional structured format, however Twitter does supply information in relation to API calls. These API calls make the retrieval of data easier for the developer and consist of multiple options such as:\n",
        "- Search for historical tweets,\n",
        "- Monitoring account activity,\n",
        "- Filter real-time tweets, \n",
        "- Build customised Direct Messages, and\n",
        "- Embedding tweets and timelines into websites (Twitter, 2019).\n",
        "\n",
        "The second problem is handling the large amounts of data made available. With this problem comes the flow-on effect of a large computational cost. The proposed method would have to not only convert the unstructured data into a structured and readable format, but it would also have to determine what is deemed a \"viral\" tweet in creating relationships to the data with the factors. These factors include the number of followers, the number of retweets and the number of mentions for each individual user or their tweets. Due to the large size of the assumed dataset, this process would take a significant amount of time to compute and process, in order to receive any output.\n",
        "\n",
        "The third problem is filtering the large dataset into a reasonable and legitimate size without placing any bias or manipulation strategies on the dataset. As the dataset is large, there would need to be some sort of filtering applied not only the contents of the tweets, but the numbers of tweets as well. The developer would need to make the decision on which the filtering occurred and what tweets would be included. This relates to the language of the tweet, the length of the tweet, the contents of the tweet, the number of followers the individual had, and what timeframe will be analysed to determine the most appropriate and accurate result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt4aGQIdNO7h",
        "colab_type": "text"
      },
      "source": [
        "###2. __Describe two approaches using data analytics to predict whether a tweet will go viral or not.__\n",
        "1. The first approach is... \n",
        "\n",
        "2. The second approach is..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVj82KzdTk-C",
        "colab_type": "text"
      },
      "source": [
        "####Current approaches\n",
        "Jenders et.al --> \n",
        "1. analyse \"obvious\" tweets and user features (based on no. followers, tweet length, no. hashtags, no. mentions) and the impact of the tweet and retweet frequency.\n",
        "2. analyse word meanings in a tweet and the retweet frequency\n",
        "- Compare between a Naive Bayes model (conditional feature independence) and a generalised linear model (avoids simplifying assumptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k8_KpSRNdXU",
        "colab_type": "text"
      },
      "source": [
        "###3. __How would you validate these approaches?__\n",
        "The most logical and appropriate way to validate these approaches would be to execute the approaches using the same datasets (training and test). There are a few methods that can be used to determine the performance of the selected approach, such as; the Holdout method, Cross-validation method and the Bootstrap method (Linkopling University, 2016).\n",
        "\n",
        "####Holdout Method\n",
        "The Holdout method splits the data into two sets of data (the training and testing datasets). The most common split for this method is 70% training, 30% testing, however if the split is not randomised, the data samples may not be an accurate representation of the approach. This method only produces an estimate of the classifier/approach and it can become more reliable if the method is repeated with various subsamples of training and testing data. This can be achieved in one of two ways; setting the seed to a specific number so that the data is split the same way either time and the end user must manually change the number to receive different results. The alternative way is to use a random seed which eiminates the manual input and potential bias within the data split. \n",
        "\n",
        "####Cross-validation Method\n",
        "The Cross-validation method avoids overlapping test sets and in most cases the datasets are put through the stratification method before the cross-validation method. Previous tests have shown that the best performing method for evaluation is the \"stratified ten-fold cross-validation method\" (Linkopling University, 2016). This method produces both an acurracy and error rate (which is derived from the average to yield the overall error estimate).\n",
        "\n",
        "####Bootstrap Method\n",
        "Unlike the Cross-validation method, the Bootstrap method uses a sampling method that replaces data to form the training set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Uhb-4fNiyd",
        "colab_type": "text"
      },
      "source": [
        "###4. __Discuss the ethical and social consequences of this study__\n",
        "When collecting data from Twitter there are several ethical and social consequences that may arise from this study. The first potential consequence would be that personal information about the individual could be collected in the data retrieval process. In the perfect scenario, any personal information would not be collected, and the tweets would be anonymised. However, quite often individuals have multiple social media accounts connected to each other and information could be collected unknowingly throughout the data collection stage. This additional collected information could de-anonymise the individual and their tweets, therefore making their personal information readily available to others without them knowing. This example raises the concern of privacy around the process of retrieving these tweets from individuals, as either approach could be used to exploit the data and information collected from the user.\n",
        "\n",
        "It is also difficult to monitor and regulate the data and information that is being collected by individuals, groups and third-party organisations that use social media applications for marketing purposes. This raises the question as to whether these approaches are ethically and socially acceptable and appropriate, as there can be several consequences for individuals if their data and information is misused, including identity theft and misuse of personal data for company gains or profits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-Gc4eEAPsH0",
        "colab_type": "text"
      },
      "source": [
        "###References\n",
        "Jenders, M., Kasneci, G. & Naumann, F. 2013, 'Analyzing and Predicting Viral Tweets', *Proceedings of the 22nd International Conference on World Wide Web*, pp. 657-664.\n",
        "\n",
        "Linkopling University, 2016, Data Mining Evaluation of a Classifier, *Subject TMN033: Introduction to Data Mining*, lecture notes, Sweden, viewed 9 October 2019, <http://staffwww.itn.liu.se/~aidvi/courses/06/dm/lectures/lec6.pdf>.\n",
        "\n",
        "Karsligil, O. 2018, *What are ethical issues in data mining?*, Quora, viewed 7 October 2019, <https://www.quora.com/What-are-ethical-issues-in-data-mining>.\n",
        "\n",
        "Sayce, D. 2019, *Number of tweets per day?*, David Sayce, viewed 7 October 2019, <https://www.dsayce.com/social-media/tweets-day/>.\n",
        "\n",
        "Twitter, 2019, *Docs - Twitter Developers*, Twitter, viewed 6 October 2019, <https://developer.twitter.com/en/docs>.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt_TpkkkBkWd",
        "colab_type": "text"
      },
      "source": [
        "###Appendix\n",
        "GitHub Respository: https://github.com/jmanning21/31005_MachineLearning_Python/blob/master/Assignment_3_12875795.ipynb\n",
        "\n",
        "Plain text: https://github.com/jmanning21/31005_MachineLearning_Python/blob/master/Assignment_3_12875795_PLAIN.txt"
      ]
    }
  ]
}